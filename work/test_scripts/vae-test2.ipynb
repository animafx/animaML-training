{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "class UnFlatten(nn.Module):\n",
    "    # NOTE: (size, x, x, x) are being computed manually as of now (this is based on output of encoder)\n",
    "    def forward(self, input, size=512): # size=128\n",
    "        return input.view(input.size(0), size, 3, 3, 3)\n",
    "        # return input.view(input.size(0), size, 6, 6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3D_VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=200, latent_dim=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            # First Convolutional Layer\n",
    "            nn.Conv3d(3, 16, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm3d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            # Second Convolutional Layer\n",
    "            nn.Conv3d(16, 32, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm3d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            # Third Convolutional Layer\n",
    "            nn.Conv3d(32, 64, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm3d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            # Fourth Convolutional Layer\n",
    "            nn.Conv3d(64, 128, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm3d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc1_mu = nn.Linear(27648, latent_dim)\n",
    "        self.fc2_sigma = nn.Linear(27648, latent_dim)\n",
    "        self.fc3_decoder = nn.Linear(latent_dim, 27648)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.BatchNorm3d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(in_channels=128, out_channels=128, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm3d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose3d(in_channels=128, out_channels=64, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm3d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose3d(in_channels=64, out_channels=32, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm3d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose3d(in_channels=32, out_channels=16, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm3d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose3d(in_channels=16, out_channels=image_channels, kernel_size=4, stride=1, padding=0), # dimensions should be as original\n",
    "            nn.BatchNorm3d(num_features=3),\n",
    "            # nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        # std = logvar.mul(0.5).exp_()\n",
    "        # eps = torch.randn(*mu.size())\n",
    "        eps = torch.rand_like(std)\n",
    "        # z = mu + std * eps\n",
    "        z = eps.mul(std).add_(mu)\n",
    "        return z\n",
    "\n",
    "    def bottleneck(self, h):\n",
    "        # print(\"[INFO] bottleneck h size:\", h.size())\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        # print(\"[INFO] h size:\", h.size()) # torch.Size([10, 27648])\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def representation(self, x):\n",
    "        return self.bottleneck(self.encoder(x))[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"[INFO] Input data shape:\", x.size())\n",
    "\n",
    "        # Step 1: compute representation (fetch it separately for later clustering)\n",
    "        z_representation = self.representation(x)\n",
    "        # print(\"[INFO] Forward z_representation:\", z_representation.size())\n",
    "        # print(\"[INFO] Reshaped latent z\", z_representation.view(z_representation.size(0), 8, 8).size())\n",
    "\n",
    "        # Step 2: call full CVAE --> encode & decode\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.fc3(z)\n",
    "        # print(\"[INFO] Latent z after dense fc:\", z.size())\n",
    "        # print(\"[INFO] mu:\", mu.size())\n",
    "        # print(\"[INFO] logvar\", logvar.size())\n",
    "\n",
    "        return self.decode(z), mu, logvar, z_representation\n",
    "    \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
